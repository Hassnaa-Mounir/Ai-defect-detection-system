{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a2aadab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras,os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8eb475eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6903 images belonging to 2 classes.\n",
      "Found 715 images belonging to 2 classes.\n",
      "Found 1725 images belonging to 2 classes.\n",
      "The folder contains 3752 files.\n",
      "The folder contains 4344 files.\n"
     ]
    }
   ],
   "source": [
    "train='C:/archive (1)/casting_data/casting_data/train'\n",
    "test='C:/archive (1)/casting_data/casting_data/test'\n",
    "\n",
    "#train='D:/Final/Graduation project/datasets/casting_data/casting_data/train'\n",
    "#test='D:/Final/Graduation project/datasets/casting_data/casting_data/test'\n",
    "\n",
    "class_names = ['def_front', 'ok_front']\n",
    "folder_names = ['def_aug', 'ok_aug']\n",
    "class_to_folder = dict(zip(range(len(class_names)), folder_names))\n",
    "\n",
    "# Define the data generators with augmentation\n",
    "trdata = ImageDataGenerator(rescale=1/255.0,\n",
    "                            rotation_range=20,\n",
    "                            zoom_range=0.05,\n",
    "                            width_shift_range=0.05,\n",
    "                            height_shift_range=0.05,\n",
    "                            shear_range=0.05,\n",
    "                            horizontal_flip=True,\n",
    "                            fill_mode=\"nearest\",\n",
    "                            validation_split=0.20)\n",
    "\n",
    "# Define the batch size and image size\n",
    "batch_size = 32\n",
    "img_size = (224, 224)\n",
    "\n",
    "# Create the training generator\n",
    "traindata = trdata.flow_from_directory(directory=train_dir,\n",
    "                                              target_size=img_size,\n",
    "                                              subset='training',\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True,\n",
    "                                              color_mode='rgb',\n",
    "                                              class_mode='categorical',\n",
    "                                              save_to_dir=train_dir,\n",
    "                                              classes=class_names)\n",
    "\n",
    "# Create the test generator\n",
    "tsdata = ImageDataGenerator(rescale=1/255.0)\n",
    "testdata = tsdata.flow_from_directory(directory=test_dir,\n",
    "                                             target_size=img_size,\n",
    "                                             color_mode='rgb',\n",
    "                                             batch_size=batch_size,\n",
    "                                             class_mode=None,\n",
    "                                             shuffle=False,\n",
    "                                             save_to_dir=test_dir,\n",
    "                                             classes=class_names)\n",
    "\n",
    "valid_generator = trdata.flow_from_directory( directory=train,\n",
    "    target_size=img_size,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "files = os.listdir('C:/archive (1)/casting_data/casting_data/train/ok_front')\n",
    "folder_length = len(files)\n",
    "print(f\"The folder contains {folder_length} files.\")\n",
    "\n",
    "num_images_to_generate = 50\n",
    "for i in range(num_images_to_generate):\n",
    "    batch = traindata.next()\n",
    "    for j in range(batch_size):\n",
    "        img = batch[0][j]\n",
    "        label = batch[1][j]\n",
    "        class_index = tf.argmax(label).numpy()\n",
    "        folder_path = os.path.join(train_dir, 'def_front' if label[0] == 1 else 'ok_front')\n",
    "        save_path = os.path.join(folder_path, f'{class_to_folder[class_index]}_{i}{j}.jpg')\n",
    "        #print(save_path)\n",
    "        tf.keras.preprocessing.image.save_img(save_path, img)\n",
    "        \n",
    "files = os.listdir('C:/archive (1)/casting_data/casting_data/train/ok_front')\n",
    "folder_length = len(files)\n",
    "print(f\"The folder contains {folder_length} files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "575a5284",
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG = keras.applications.vgg16.VGG16(input_shape=(224,224,3), include_top = False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31a74415",
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG.trainable = False\n",
    "#Not train the front 13 layers, train only last two layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19dd92fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    VGG,\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(units=256, activation=\"relu\"),\n",
    "    keras.layers.Dense(units=256, activation=\"relu\"),\n",
    "    keras.layers.Dense(units=2, activation=\"softmax\"), \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5364de91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 25088)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               6422784   \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,203,778\n",
      "Trainable params: 6,489,090\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547e8d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(traindata , steps_per_epoch = 196   , epochs = 5 , #traindata.n \n",
    "                           validation_data = valid_generator, validation_steps = 49)\n",
    "model.save('vggclf_v5.h5')\n",
    "\n",
    "\n",
    "# Your input ran out of data; interrupting training. Make sure that your \n",
    "#dataset or generator can generate at least `steps_per_epoch * epochs` batches \n",
    "# image data generator should generate at least 195*10 = 1950 so if i set batch size to 32 so i need a \n",
    "# 6230 / 32 = 195 batch to go through entire data set  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33e2692",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(valid_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8e4a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(hist.history['accuracy'])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title(\"model accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Accuracy\",\"Validation Accuracy\",\"loss\",\"Validation Loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0242cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(testdata,steps = testdata.n, verbose = 1)\n",
    "\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34403c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "img = keras.utils.load_img(\"D:/Final/Graduation project/datasets/12.png\",target_size=(224,224))\n",
    "img = np.asarray(img)\n",
    "\n",
    "\n",
    "plt.imshow(img)\n",
    "img = np.expand_dims(img, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ed676f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "saved_model = load_model(\"vggclf_v5.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a3db48",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.predict(img)\n",
    "if output[0][0] > output[0][1]:\n",
    "    print(\"defected\")\n",
    "    print(output)\n",
    "else:\n",
    "    print(\"good\")\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556c2a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "  \n",
    "# Save the trained model as a pickle string.\n",
    "saved_model = pickle.dumps(model)\n",
    "model_from_pickle = pickle.loads(saved_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
